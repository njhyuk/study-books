# 2장 주변친구

1장의 사업장 위치와 다르게 주변 친구의 위치는 자주 바뀐다
## 1단계: 문제 이해 범위 및 설계범위 확정
* 주변에 있다의 범위 : 5마일(설정가능)
* 두 사용자 사이의 직선거리라고 가정 (강이 있어도 무시)
* 10억명 사용자중 10%가 사용하는 기능으로 가정
* 이동 이력 보관 필요함
* 10분 이상 비활성화 상태이면 주변친구에서 사라지도록 가정
* 데이터 보호법 고려하지 않는것으로 가정
### 기능 요구사항
* 사용자는 앱에서 주변 친구 확인 가능
* 해당 친구까지의 거리, 해당 정보가 마지막으로 갱신된 시각 표시
* 친구 목록은 몇초마다 한번씩 갱신
### 비기능 요구사항
낮은 지연시간, 안정성, 결과적 일관성
### 개략적 규모 추정
* 1억 DAU
* 동시 접속 사용자 10% * 1억 = 천만
* 30초마다 위치 전송
* QPS = 천만/30초=334,000
> 매번 보면서 QPS를 따져서 아키텍처가 어떻게 달라지는지, 가늠하기 어렵다
> 확장 가능하게 구현하는게 방법이지 않을까 싶다
## 2단계: 개략적 설계안 제시 및 동의 구하기
클라이언트가 위치를 전송해야해서 HTTP를 못쓸 수 있다는것 감안
### 개략적 설계안
* 이론적으로 P2P 방식으로 해결 가능한 문제 [그림2.2]
* 활성 상태인 근방 모든 친구와 통신 상태를 유지하면 되는 것
* 그러나 모바일은 통신 연결 상태와 전력때문에 실용적이지 않아 백엔드로 구현하기로 함
> 네이버 치지직이나, 아프리카가 쓰는 P2P 방식을 어떻게 구현하는지 궁금해짐
* 백엔드의 역할
  * 모든 활성 상태 사용자의 위치 변화 내역을 수신한다
  * 사용자 위치 변경 내역을 수신할 때마다, 관련 친구들에게 변경 내역 갱신
  * 특정 임계치보다 너무 먼 경우 변경내역 전송 안하도록
  * 문제점
    * 천만명이 30초마다 갱신하면 자체 DDOS 발생
### 설계안 (소규모용)
* 로드밸런서, Restful API 서버, 웹소켓 서버
* 웹소켓 서버
  * 친구 위치 정보 변경을 거의 실시간에 가깝게 처리, 유상태 서버 클러스터
  * 클라이언트의 초기화, 모든 주변 친구 위치를 클라이언트에 전송 담당
#### 레디스 위치 정보 캐시
* 가장 최근 위치 정보를 캐시
* TTL 필드로 활성 상태 만료 관리
* 캐시 정보 갱신시에는 TTL도 갱신
#### 사용자 데이터베이스
* 관계형이나 Nosql 다 가능
* 사용자 데이터 및 친구 관계 정보.. 1:N이라 간단한듯
#### 위치 이동 이력 데이터베이스
* 사용자의 위치 변동 이력 보관 (추후 머신러닝 학습을 위한 용도)
#### 레디스 펍/섭 서버
* 웹소켓 서버르 통해 수신한 위치 정보 이벤트를 펍/섭 채널에 발행
* 사용자별 채널로 pub, 사용자의 친구들이 sub
#### 주기적 위치 갱신 
[그림2.7]
### API 설계
* [서버 API] 주기적인 위치 정보 갱신
* [클라이언트 API] 클라이언트가 갱신된 친구 위치를 수신하는데 사용할 API
* [서버 API] 웹소켓 초기화 API
* [클라이언트 API] 새 친구 구독 API
* [클라이언트 API]  구독 해지 API

### 데이터 모델
* 위치 정보 캐시 (레디스)
  * key : 사용자 ID : 값 : {위도, 경도, 시각}
* DB를 사용하지 않은 이유
  * 주변 친구 기능은 사용자의 현재 위치만 사용하기 때문 (하나만 보관하고 휘발)
* 위치 이동 이력 DB
  * user_id, 위도, 경도, 시간 필드
  * 막대한 쓰기 연산 부하 감당을 위해 카산드라를 채택

## 3단계: 상세 설계
### 규모 확장성
* API서버(무상태) : CPU 사용률 또는 I/O 상태에 따라 오토 스케일링
* 웹소켓 서버(상태)
  * 노드를 제거하기 전에 기존에 연결된 클라이언트들이 있음
  * 로드밸런서에 연결 종료중 상태 전달
  * 새로운 커넥션은 맺지 않음
  * 모든 커넥션이 종료되면 노드에서 제거
### 클라이언트 초기화
* 웹소켓은 "지속성 연결"을 맺어야함
  * 대부분의 현대적 프로그래밍 언어는 지속성 연결 유지 비용이 적음
* 웹소켓 연결 핸들러의 작업은? 👇
* 캐시에 보관된 사용자 위치 갱신
* 위치는 변수에 저장해둠
* DB를 뒤져 해당 사용자의 모든 친구 정보 조회
* 위치정보 캐시에서 모든 친구의 위치를 가져옴
* 캐시에 보관하는 TTL = 비활성화 타임아웃 시간
  * 비활성화 친구의 위치는 캐시에 없다
* 웹소켓 서버는 친구별 위치와 사용자 사이의 거리 계산
  * 해당 위치가 마지막으로 확인된 시각 반환
* 웹소켓 서버는 각 친구의 레디스 서버 펍/섭 채널 구독
* 사용자의 현재 위치 -> 레디스 펍/섭 서버의 전용 채널로 모든 친구에게 전송

### 사용자 데이터베이스
* 사용자 상세정보 데이터, 친구 관계 데이터 취급
* 한대의 관계형으로 감당 불가 (사용자 ID로 샤딩 필요)

### 위치 정보 캐시
* 각 항목의 키에는 TTL 설정
* 해당 사용자의 위치 정보가 갱신될 때마다 초기화됨
* 캐시할 데이터는 쉽게 샤딩 가능
  * 각 사용자의 위치 정보는 독립적

### 레디스 펍/섭 서버
* 펍/섭 서버를 위치 변경 내역 메시지 라우팅 계층으로 활용했음
* 레디스 펍/섭 채널 비용이 아주 저렴하기 때문
* 구독자가 없는 채널로 전송된 메시지는 버려짐, 부담없음
* 구독자 관계를 추적하기 위한 자료 구조
  * 해시 테이블, 연결 리스트
  * 소량의 메모리만 사용
* 오프라인 사용자라 변경이 없는 채널
  * CPU 자원 사용 안함
* 주변 친구 기능을 활용하는 모든 사용자에게 채널 하나씩 부여

### 얼마나 많은 레디스 펍/섭 서버가 필요한가?

#### 메모리 사용량
* 채널의 수는 1억 개임 (10억 사용자의 10%)
* 해시 테이블과 연결 리스트는 20바이트로 가정
* 1억*20바이트=200GB 메모리
* 100GB 메모리 서버면 2대 필요

#### CPU 사용량
* 위치 정보 업데이트 양은 초당 1400만건
* 서버 한대로 감당 가능한 수는 십만건이라고 가정
* 1400만/십만=140대
* 본 설계안의 결론
  * 병목은 메모리가 아닌 CPU 사용량
  * 분산 레디스 펍/섭 클러스터 필요

#### 분산 레디스 펍/섭 서버 클러스터
* 서버에는 필연적으로 장애가 생기기 마련
* 서비스 탐색 컴포넌트 도입으로 문제 해결
  * etcd, 주키퍼
* 가용한 서버 목록을 유지하는 소규모 키-값 저장소 (해시링)
  * [p_1, p_2, p_3, p_4]
* 키에 매달린 값에는 활성 상태의 모든 레디스 펍/섭 서버로 구성된 해시링 보관
* [그림 2.9] 에 따르면 채널 2는 레디스 펍/섭 서버 1번에서 관리
* [그림 2.10]
  * 웹소켓 서버는 해시 링을 참조하여, 레디스 펍/섭 서버를 선정함
  * 웹소켓 서버는 해당 서버가 관리하는 채널에 위치정보 변경 내역 발행

#### 레디스 펍/섭 클러스터 규모
* 채널의 모든 구독자에게 전송되고 나면 바로 삭제 (무상태 특성)
* 펍/섭 서버는 채널에 대한 상태 정보 보관 (유상태 특성)
  * 각 채널의 구독자 목록은 그 상태 정보의 핵심적 부분
  * 펍/섭 서버를 교체하거나 해시 링에서 제거하는 경우
    * 채널은 다른 서버로 이동시켜야함
    * 해당 채널의 모든 구독자에게 알려야함
* 유상태 서버 클러스터로 취급하는것이 바람직함
  * 즉, 늘리고 줄이는게 부담
  * 오버 프로비지닝 해두는게 좋다
* 클러스터 크기를 조정하면, 갱신됨을 알리고 엄청난 재구독 현상
  * 클라이언트가 보내는 위치 정보 메시지의 처리가 누락 될 수 있음
* 클러스터 조정은 부하가 낮은 시간에 하는게 좋음
* 조정 방법
  * 새로운 링 크기 계산, 크기가 늘어나면 새 서버 준비
  * 해시 링의 키에 매달린 값을 갱신

#### 운영 고려사항
* 새 서버 교체는 크기 조정보다 쉬움 (교체되는 서버의 채널만 손보면 됨)
* 모니터링 소프트웨어가 펍/섭 서버 장애 감지

### 주변의 임의 사용자 요구사항 추가
* 주변 사용자를 무작위로 보여주는 기능 구현
* 지오 해시에 따라 구축된 펍/섭 채널 풀을 두기
  * [그림2-12] 네개의 지오해시 격자로 나눠서 채널 하나씩 만들어 두기

#### 레디스 펍/섭 외의 대안
* 얼랭은 이 문제에 대한 해결책
  * 고도로 분산된 병렬 애플리케이션을 위해 고안된 언어
  * 가장 작은 얼랭 프로세스는 300바이트의 메모리만 사용
  * 서버 한대로 수백만 프로세스 가능
  * 천만 사용자 각각 얼랭 프로세스로 모델링 가능
    * 웹 소켓 서버 -> 얼랭
    * 레디스 펍/섭 클러스터 -> 분산 얼랭 애프리케이션
  * 얼랭엔 구독 기능을 내장하고 있어 구현이 쉬움

> laravel-excel 을 이용해서 레디스 펍/섭 으로 엑셀 대량 업로드를 구현했던 경험이 있음
> 책의 예시와 다르게 유상태 특성이 있어, 서버마다 엑셀 row 를 나눠서 처리함
> 면접에서 어떻게 state 를 관리하는지 제대로 설명하지 못해 떨어진 경험
> 펍/섭의 단점은 이러한 유상태의 특성을 지닌 경우, 구독자가 여러명일때 하나의 구독자에만 전달한다거나, 나눠서 처리하는 구현이 까다로워 질 수 있음

## 4단계: 마무리
* 웹소켓 : 클라이언트와 서버 사이의 실시간 통신
* 레디스 : 위치 데이터의 빠른 읽기/쓰기
* 레디스 펍/섭 : 모든 온라인 친구에게 위치변경 내역 전달, 라우팅 계층